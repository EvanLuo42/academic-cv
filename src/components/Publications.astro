---
import Publication from "./Publication.astro";
const authors = [
  "Long Chen",
  "Oleg Sinavski",
  "Jan Hunermann"
]
import cover from "../assets/cover.jpg"
---

<div class="flex flex-col items-center space-y-10 xl:items-start xl:flex-row xl:space-x-20 justify-center py-[110px]">
  <h1 class="text-4xl xl:text-5xl">Recent Publications</h1>
  <div class="flex flex-col space-y-10">
    <Publication
      title="One Thousand and One Hours: Self-driving Motion Prediction Dataset"
      abstract="Large Language Models (LLMs) have shown promise in the autonomous driving sector, particularly in generalization and interpretability. We introduce a unique object-level multimodal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. We also present a new dataset of 160k QA pairs derived from 10k driving scenarios, paired with high quality control commands collected with RL agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data. We also introduce an evaluation metric for Driving QA and demonstrate our LLM-driver’s proficiency in interpreting driving scenarios, answering questions, and decision-making. Our findings highlight the potential of LLM-based driving action generation in comparison to traditional behavioral cloning. We make our benchmark, datasets, and model available for further exploration."
      link="/"
      authors={authors}
      date="December 2024"
      place="the 18th European Conference on Computer Vision (ECCV 2024)"
      resources={[
        {name: "PDF", link: "https://arxiv.org/pdf/2312.14115.pdf"}
      ]}
      image={cover}
    />
    <Publication
      title="One Thousand and One Hours: Self-driving Motion Prediction Dataset"
      abstract="Large Language Models (LLMs) have shown promise in the autonomous driving sector, particularly in generalization and interpretability. We introduce a unique object-level multimodal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. We also present a new dataset of 160k QA pairs derived from 10k driving scenarios, paired with high quality control commands collected with RL agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data. We also introduce an evaluation metric for Driving QA and demonstrate our LLM-driver’s proficiency in interpreting driving scenarios, answering questions, and decision-making. Our findings highlight the potential of LLM-based driving action generation in comparison to traditional behavioral cloning. We make our benchmark, datasets, and model available for further exploration."
      link="/"
      authors={authors}
      date="December 2024"
      place="the 18th European Conference on Computer Vision (ECCV 2024)"
      resources={[
        {name: "PDF", link: "https://arxiv.org/pdf/2312.14115.pdf"}
      ]}
      image={cover}
    />
    <Publication
      title="One Thousand and One Hours: Self-driving Motion Prediction Dataset"
      abstract="Large Language Models (LLMs) have shown promise in the autonomous driving sector, particularly in generalization and interpretability. We introduce a unique object-level multimodal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. We also present a new dataset of 160k QA pairs derived from 10k driving scenarios, paired with high quality control commands collected with RL agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data. We also introduce an evaluation metric for Driving QA and demonstrate our LLM-driver’s proficiency in interpreting driving scenarios, answering questions, and decision-making. Our findings highlight the potential of LLM-based driving action generation in comparison to traditional behavioral cloning. We make our benchmark, datasets, and model available for further exploration."
      link="/"
      authors={authors}
      date="December 2024"
      place="the 18th European Conference on Computer Vision (ECCV 2024)"
      resources={[
        {name: "PDF", link: "https://arxiv.org/pdf/2312.14115.pdf"}
      ]}
      image={cover}
    />
  </div>
</div>